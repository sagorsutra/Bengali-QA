{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7836912,"sourceType":"datasetVersion","datasetId":4593827},{"sourceId":8580104,"sourceType":"datasetVersion","datasetId":5131216}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-01T15:39:16.217025Z","iopub.execute_input":"2024-10-01T15:39:16.217586Z","iopub.status.idle":"2024-10-01T15:39:16.228155Z","shell.execute_reply.started":"2024-10-01T15:39:16.217548Z","shell.execute_reply":"2024-10-01T15:39:16.226936Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/eval-dataset/Eval_Dataset.xlsx\n/kaggle/input/banglanews24/banglanews24.parquet\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport re,json,nltk\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ndata = pd.read_parquet('/kaggle/input/banglanews24/banglanews24.parquet')\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:40:46.827873Z","iopub.execute_input":"2024-10-01T15:40:46.828234Z","iopub.status.idle":"2024-10-01T15:41:08.196742Z","shell.execute_reply.started":"2024-10-01T15:40:46.828205Z","shell.execute_reply":"2024-10-01T15:41:08.195510Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-10-01 15:40:50.644773: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-10-01 15:40:50.644919: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-10-01 15:40:50.801632: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                Text  \\\n0  চাঁদপুর: প্রাচীন বন্দর নগরী ও তিন নদীর মোহনায় ...   \n1  ফেনী: ঢাকা-চট্রগ্রাম মহাসড়কের ফেনীর মহিপাল বাস...   \n2  মৌলভীবাজার: জেলার শ্রীমঙ্গলে দেশিয় পদ্ধতিতে তৈ...   \n3  সিলেট: সিলেটে একটি পোল্ট্রি অ্যান্ড ফিসারিজ কো...   \n4  ঢাকা: নারায়ণগঞ্জের রূপগঞ্জে জাতীয় নিরাপত্তা গো...   \n5  ব্রাহ্মণবাড়িয়া: জেলায় কনটেইনার ট্রেনের বগি লাই...   \n6  ঢাকা: রাজধানীর ওয়ারী থানার সামনে ককটেল বিস্ফো...   \n7  বান্দরবান: বান্দরবানের লামায় জমি চাষের জন্য শ্...   \n8  ঢাকা: ভয়াবহ মাদক বুপ্রেনরফিনসহ দুই কারবারিকে র...   \n9  ঢাকা: রাজধানীর যাত্রাবাড়ী থানার সামনে একটি বা...   \n\n                             Timestamp  \\\n0  আপডেট: ২১৪৪ ঘণ্টা, নভেম্বর ১৯, ২০২৩   \n1  আপডেট: ২১৪৩ ঘণ্টা, নভেম্বর ১৯, ২০২৩   \n2  আপডেট: ০২১৩ ঘণ্টা, নভেম্বর ২০, ২০২৩   \n3  আপডেট: ০০০৬ ঘণ্টা, নভেম্বর ২০, ২০২৩   \n4  আপডেট: ০০৫৮ ঘণ্টা, নভেম্বর ২০, ২০২৩   \n5  আপডেট: ০০১৭ ঘণ্টা, নভেম্বর ২০, ২০২৩   \n6  আপডেট: ২৩৫২ ঘণ্টা, নভেম্বর ১৯, ২০২৩   \n7  আপডেট: ২১৩৫ ঘণ্টা, নভেম্বর ১৯, ২০২৩   \n8  আপডেট: ০১৫৯ ঘণ্টা, নভেম্বর ২০, ২০২৩   \n9  আপডেট: ২২৫০ ঘণ্টা, নভেম্বর ১৯, ২০২৩   \n\n                                                 URL  \n0  https://www.banglanews24.com/national/news/bd/...  \n1  https://www.banglanews24.com/national/news/bd/...  \n2  https://www.banglanews24.com/national/news/bd/...  \n3  https://www.banglanews24.com/national/news/bd/...  \n4  https://www.banglanews24.com/national/news/bd/...  \n5  https://www.banglanews24.com/national/news/bd/...  \n6  https://www.banglanews24.com/national/news/bd/...  \n7  https://www.banglanews24.com/national/news/bd/...  \n8  https://www.banglanews24.com/national/news/bd/...  \n9  https://www.banglanews24.com/national/news/bd/...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Timestamp</th>\n      <th>URL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>চাঁদপুর: প্রাচীন বন্দর নগরী ও তিন নদীর মোহনায় ...</td>\n      <td>আপডেট: ২১৪৪ ঘণ্টা, নভেম্বর ১৯, ২০২৩</td>\n      <td>https://www.banglanews24.com/national/news/bd/...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ফেনী: ঢাকা-চট্রগ্রাম মহাসড়কের ফেনীর মহিপাল বাস...</td>\n      <td>আপডেট: ২১৪৩ ঘণ্টা, নভেম্বর ১৯, ২০২৩</td>\n      <td>https://www.banglanews24.com/national/news/bd/...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>মৌলভীবাজার: জেলার শ্রীমঙ্গলে দেশিয় পদ্ধতিতে তৈ...</td>\n      <td>আপডেট: ০২১৩ ঘণ্টা, নভেম্বর ২০, ২০২৩</td>\n      <td>https://www.banglanews24.com/national/news/bd/...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>সিলেট: সিলেটে একটি পোল্ট্রি অ্যান্ড ফিসারিজ কো...</td>\n      <td>আপডেট: ০০০৬ ঘণ্টা, নভেম্বর ২০, ২০২৩</td>\n      <td>https://www.banglanews24.com/national/news/bd/...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ঢাকা: নারায়ণগঞ্জের রূপগঞ্জে জাতীয় নিরাপত্তা গো...</td>\n      <td>আপডেট: ০০৫৮ ঘণ্টা, নভেম্বর ২০, ২০২৩</td>\n      <td>https://www.banglanews24.com/national/news/bd/...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ব্রাহ্মণবাড়িয়া: জেলায় কনটেইনার ট্রেনের বগি লাই...</td>\n      <td>আপডেট: ০০১৭ ঘণ্টা, নভেম্বর ২০, ২০২৩</td>\n      <td>https://www.banglanews24.com/national/news/bd/...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ঢাকা: রাজধানীর ওয়ারী থানার সামনে ককটেল বিস্ফো...</td>\n      <td>আপডেট: ২৩৫২ ঘণ্টা, নভেম্বর ১৯, ২০২৩</td>\n      <td>https://www.banglanews24.com/national/news/bd/...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>বান্দরবান: বান্দরবানের লামায় জমি চাষের জন্য শ্...</td>\n      <td>আপডেট: ২১৩৫ ঘণ্টা, নভেম্বর ১৯, ২০২৩</td>\n      <td>https://www.banglanews24.com/national/news/bd/...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ঢাকা: ভয়াবহ মাদক বুপ্রেনরফিনসহ দুই কারবারিকে র...</td>\n      <td>আপডেট: ০১৫৯ ঘণ্টা, নভেম্বর ২০, ২০২৩</td>\n      <td>https://www.banglanews24.com/national/news/bd/...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ঢাকা: রাজধানীর যাত্রাবাড়ী থানার সামনে একটি বা...</td>\n      <td>আপডেট: ২২৫০ ঘণ্টা, নভেম্বর ১৯, ২০২৩</td>\n      <td>https://www.banglanews24.com/national/news/bd/...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Total Reviews:\",len(data))","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:54:49.951595Z","iopub.execute_input":"2024-10-01T15:54:49.952323Z","iopub.status.idle":"2024-10-01T15:54:49.958503Z","shell.execute_reply.started":"2024-10-01T15:54:49.952287Z","shell.execute_reply":"2024-10-01T15:54:49.957343Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Total Reviews: 143642\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport re\n# !pip install pandas numpy torch transformers tensorflow\n\n\n# Define the process_comments function\ndef process_text(Text):\n    Text = re.sub('[^\\u0980-\\u09FF]', ' ', str(Text))  \n    return Text\n  \nsubset_size = int(len(data) * 0.002)\nsubset_data = data.head(subset_size) # Take a random 10% subset of the data\n\nsubset_data.loc[:, 'Answer'] = ''\n\n# Now, apply the cleaning function to the subset data\nsubset_data['cleaned'] = subset_data['Text'].apply(process_text)\n\nprint(\"Total Reviews:\", len(data))\nprint(\"Subset Size:\", len(subset_data))\nprint(subset_data[['Text', 'cleaned', 'Answer']])\nprint(data.loc[35, 'Text'])","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:54:52.127159Z","iopub.execute_input":"2024-10-01T15:54:52.127526Z","iopub.status.idle":"2024-10-01T15:54:52.175062Z","shell.execute_reply.started":"2024-10-01T15:54:52.127499Z","shell.execute_reply":"2024-10-01T15:54:52.173803Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Total Reviews: 143642\nSubset Size: 287\n                                                  Text  \\\n0    চাঁদপুর: প্রাচীন বন্দর নগরী ও তিন নদীর মোহনায় ...   \n1    ফেনী: ঢাকা-চট্রগ্রাম মহাসড়কের ফেনীর মহিপাল বাস...   \n2    মৌলভীবাজার: জেলার শ্রীমঙ্গলে দেশিয় পদ্ধতিতে তৈ...   \n3    সিলেট: সিলেটে একটি পোল্ট্রি অ্যান্ড ফিসারিজ কো...   \n4    ঢাকা: নারায়ণগঞ্জের রূপগঞ্জে জাতীয় নিরাপত্তা গো...   \n..                                                 ...   \n282  ঢাকা: গবেষণাপত্রে জালিয়াতির অভিযোগে কোরিয়াভিত্...   \n283  ঢাকা: অক্টোবরের ২৮ তারিখ থেকে ১৩ নভেম্বর পর্যন...   \n284  ঢাকা: ঢাকা মেডিকেল কলেজ (ঢামেক) হাসপাতালের বহি...   \n285  খাগড়াছড়ি: প্রধানমন্ত্রী শেখ হাসিনা খাগড়াছড়ির র...   \n286  ঢাকা: আগামী দ্বাদশ জাতীয় সংসদ নির্বাচনের তফসিল...   \n\n                                               cleaned Answer  \n0    চাঁদপুর  প্রাচীন বন্দর নগরী ও তিন নদীর মোহনায় ...         \n1    ফেনী  ঢাকা চট্রগ্রাম মহাসড়কের ফেনীর মহিপাল বাস...         \n2    মৌলভীবাজার  জেলার শ্রীমঙ্গলে দেশিয় পদ্ধতিতে তৈ...         \n3    সিলেট  সিলেটে একটি পোল্ট্রি অ্যান্ড ফিসারিজ কো...         \n4    ঢাকা  নারায়ণগঞ্জের রূপগঞ্জে জাতীয় নিরাপত্তা গো...         \n..                                                 ...    ...  \n282  ঢাকা  গবেষণাপত্রে জালিয়াতির অভিযোগে কোরিয়াভিত্...         \n283  ঢাকা  অক্টোবরের ২৮ তারিখ থেকে ১৩ নভেম্বর পর্যন...         \n284  ঢাকা  ঢাকা মেডিকেল কলেজ  ঢামেক  হাসপাতালের বহি...         \n285  খাগড়াছড়ি  প্রধানমন্ত্রী শেখ হাসিনা খাগড়াছড়ির র...         \n286  ঢাকা  আগামী দ্বাদশ জাতীয় সংসদ নির্বাচনের তফসিল...         \n\n[287 rows x 3 columns]\nগাজীপুর: গাজীপুরের শ্রীপুর উপজেলায় পুকুরের পানিতে ডুবে দুই ভাইয়ের মৃত্যু হয়েছে। রোববার (১৯ নভেম্বর) দুপুরে উপজেলার বরমী ইউনিয়নের রিফজিপাড়ায় এ ঘটনা ঘটে। \n\n\n\n\n \nমৃতরা হলো- রিফজিপাড়ার আল-আমিনের দুই ছেলে তামিম হোসেন (৭) ও ইসমাইল হোসেন (৪)।  \nনিহতদের স্বজন ও এলাকাবাসী জানায়, দুপুরে বাড়ির পাশে অন্য শিশুদের সঙ্গে খেলা করছিল তামিম ও ইসমাইল। খেলার এক পর্যায়ে ইসমাইল পাশের একটি পুকুরে পড়ে ডুবে যায়। এসময় তামিম তাকে উদ্ধারের চেষ্টা করে। এক পর্যায়ে তামিমও পুকুরে পড়ে ডুবে যায়। কিছুক্ষণ পর এক শিশু ইসমাইলের মরদেহ পুকুরের পানিতে ভাসতে দেখে চিৎকার শুরু করে। পরে স্থানীয়রা ইসমাইলকে মৃত অবস্থায় উদ্ধার করে। এসময় নিখোঁজ ছিল তামিম। বিকেল সাড়ে ৩টার দিকে অনেক খোঁজাখুঁজির পর পুকুর থেকে তামিমকে মৃত অবস্থায় উদ্ধার করা হয়।  \nশ্রীপুর থানার ভারপ্রাপ্ত কর্মকর্তা (ওসি) আবুল ফজল মোহাম্মদ নাসিম জানান, খবর পেয়ে ঘটনাস্থলে পুলিশ পাঠানো হয়েছে। ঘটনাটি তদন্ত করে দেখা হচ্ছে। এ ব্যাপারে আইনগত ব্যবস্থা প্রক্রিয়াধীন।  \nবাংলাদেশ সময়: ১৮৪২ ঘণ্টা, নভেম্বর ১৯, ২০২৩\nআরএস/আরবি\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/3779035115.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset_data.loc[:, 'Answer'] = ''\n/tmp/ipykernel_33/3779035115.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset_data['cleaned'] = subset_data['Text'].apply(process_text)\n","output_type":"stream"}]},{"cell_type":"code","source":"original_subset_data = subset_data.copy()\n\nsubset_data['cleaned'].fillna('', inplace=True)\n\n# Calculate the length of each review and remove reviews with the least words\nsubset_data['length'] = subset_data['cleaned'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\nsubset_data = subset_data.loc[subset_data['length'] > 1].reset_index(drop=True)\n\nremoved_indices = original_subset_data.index.difference(subset_data.index)\n\nprint(\"Subset Size:\", len(subset_data))\nprint(\"After Cleaning:\")\nprint(\"Removed {} Small Reviews\".format(len(data) - len(subset_data)))\nprint(\"Total Reviews:\", len(subset_data))\nprint(subset_data[['Text', 'cleaned']])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:54:56.704887Z","iopub.execute_input":"2024-10-01T15:54:56.705281Z","iopub.status.idle":"2024-10-01T15:54:56.734359Z","shell.execute_reply.started":"2024-10-01T15:54:56.705250Z","shell.execute_reply":"2024-10-01T15:54:56.732962Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Subset Size: 287\nAfter Cleaning:\nRemoved 143355 Small Reviews\nTotal Reviews: 287\n                                                  Text  \\\n0    চাঁদপুর: প্রাচীন বন্দর নগরী ও তিন নদীর মোহনায় ...   \n1    ফেনী: ঢাকা-চট্রগ্রাম মহাসড়কের ফেনীর মহিপাল বাস...   \n2    মৌলভীবাজার: জেলার শ্রীমঙ্গলে দেশিয় পদ্ধতিতে তৈ...   \n3    সিলেট: সিলেটে একটি পোল্ট্রি অ্যান্ড ফিসারিজ কো...   \n4    ঢাকা: নারায়ণগঞ্জের রূপগঞ্জে জাতীয় নিরাপত্তা গো...   \n..                                                 ...   \n282  ঢাকা: গবেষণাপত্রে জালিয়াতির অভিযোগে কোরিয়াভিত্...   \n283  ঢাকা: অক্টোবরের ২৮ তারিখ থেকে ১৩ নভেম্বর পর্যন...   \n284  ঢাকা: ঢাকা মেডিকেল কলেজ (ঢামেক) হাসপাতালের বহি...   \n285  খাগড়াছড়ি: প্রধানমন্ত্রী শেখ হাসিনা খাগড়াছড়ির র...   \n286  ঢাকা: আগামী দ্বাদশ জাতীয় সংসদ নির্বাচনের তফসিল...   \n\n                                               cleaned  \n0    চাঁদপুর  প্রাচীন বন্দর নগরী ও তিন নদীর মোহনায় ...  \n1    ফেনী  ঢাকা চট্রগ্রাম মহাসড়কের ফেনীর মহিপাল বাস...  \n2    মৌলভীবাজার  জেলার শ্রীমঙ্গলে দেশিয় পদ্ধতিতে তৈ...  \n3    সিলেট  সিলেটে একটি পোল্ট্রি অ্যান্ড ফিসারিজ কো...  \n4    ঢাকা  নারায়ণগঞ্জের রূপগঞ্জে জাতীয় নিরাপত্তা গো...  \n..                                                 ...  \n282  ঢাকা  গবেষণাপত্রে জালিয়াতির অভিযোগে কোরিয়াভিত্...  \n283  ঢাকা  অক্টোবরের ২৮ তারিখ থেকে ১৩ নভেম্বর পর্যন...  \n284  ঢাকা  ঢাকা মেডিকেল কলেজ  ঢামেক  হাসপাতালের বহি...  \n285  খাগড়াছড়ি  প্রধানমন্ত্রী শেখ হাসিনা খাগড়াছড়ির র...  \n286  ঢাকা  আগামী দ্বাদশ জাতীয় সংসদ নির্বাচনের তফসিল...  \n\n[287 rows x 2 columns]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/3605439447.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  subset_data['cleaned'].fillna('', inplace=True)\n/tmp/ipykernel_33/3605439447.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset_data['cleaned'].fillna('', inplace=True)\n/tmp/ipykernel_33/3605439447.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset_data['length'] = subset_data['cleaned'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport pyarrow.parquet as pq\nimport re\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nimport torch\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\n\nbnbert_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\nsubset_data['tokens'] = ''\n\n# Tokenize the cleaned text\nsubset_data['tokens'] = subset_data['cleaned'].apply(lambda x: bnbert_tokenizer.tokenize(x))\n\n# Print the DataFrame with tokenized text\nprint(subset_data[['cleaned', 'tokens']])\ntokens = subset_data['tokens']\nprint(tokens)\nprint(subset_data.loc[1, 'tokens'])\n\n# Vectorize the cleaned texts using TF-IDF\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(subset_data['cleaned'])\n\nprint(\"TF-IDF matrix shape:\", tfidf_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:54:59.525152Z","iopub.execute_input":"2024-10-01T15:54:59.525644Z","iopub.status.idle":"2024-10-01T15:55:06.305345Z","shell.execute_reply.started":"2024-10-01T15:54:59.525605Z","shell.execute_reply":"2024-10-01T15:55:06.304159Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff863a647c2c4745a99e289aae75bb33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/2.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86cc17d19d9d4a50839d355ada908466"}},"metadata":{}},{"name":"stdout","text":"                                               cleaned  \\\n0    চাঁদপুর  প্রাচীন বন্দর নগরী ও তিন নদীর মোহনায় ...   \n1    ফেনী  ঢাকা চট্রগ্রাম মহাসড়কের ফেনীর মহিপাল বাস...   \n2    মৌলভীবাজার  জেলার শ্রীমঙ্গলে দেশিয় পদ্ধতিতে তৈ...   \n3    সিলেট  সিলেটে একটি পোল্ট্রি অ্যান্ড ফিসারিজ কো...   \n4    ঢাকা  নারায়ণগঞ্জের রূপগঞ্জে জাতীয় নিরাপত্তা গো...   \n..                                                 ...   \n282  ঢাকা  গবেষণাপত্রে জালিয়াতির অভিযোগে কোরিয়াভিত্...   \n283  ঢাকা  অক্টোবরের ২৮ তারিখ থেকে ১৩ নভেম্বর পর্যন...   \n284  ঢাকা  ঢাকা মেডিকেল কলেজ  ঢামেক  হাসপাতালের বহি...   \n285  খাগড়াছড়ি  প্রধানমন্ত্রী শেখ হাসিনা খাগড়াছড়ির র...   \n286  ঢাকা  আগামী দ্বাদশ জাতীয় সংসদ নির্বাচনের তফসিল...   \n\n                                                tokens  \n0    [চাদ, ##পর, পরা, ##চীন, বন, ##দর, নগরী, ও, তিন...  \n1    [ফেনী, ঢাকা, চট, ##র, ##গরা, ##ম, মহা, ##সড, #...  \n2    [[UNK], জেলার, শর, ##ীম, ##ঙ, ##গলে, দেশি, ##য...  \n3    [সিলেট, সিলেটে, একটি, পে, ##াল, ##টরি, অ, ##যা...  \n4    [ঢাকা, না, ##রা, ##য, ##ণ, ##গ, ##ঞ, ##জের, র,...  \n..                                                 ...  \n282  [ঢাকা, গবেষণা, ##পত, ##রে, জালি, ##যা, ##তির, ...  \n283  [ঢাকা, অক, ##টে, ##াব, ##রে, ##র, [UNK], তারিখ...  \n284  [ঢাকা, ঢাকা, মেডিকেল, কলেজ, ঢামেক, হাসপাতালের,...  \n285  [খাগ, ##ডা, ##ছ, ##ডি, পর, ##ধান, ##মন, ##তরী,...  \n286  [ঢাকা, আগামী, দ, ##বাদ, ##শ, জাতী, ##য, সংসদ, ...  \n\n[287 rows x 2 columns]\n0      [চাদ, ##পর, পরা, ##চীন, বন, ##দর, নগরী, ও, তিন...\n1      [ফেনী, ঢাকা, চট, ##র, ##গরা, ##ম, মহা, ##সড, #...\n2      [[UNK], জেলার, শর, ##ীম, ##ঙ, ##গলে, দেশি, ##য...\n3      [সিলেট, সিলেটে, একটি, পে, ##াল, ##টরি, অ, ##যা...\n4      [ঢাকা, না, ##রা, ##য, ##ণ, ##গ, ##ঞ, ##জের, র,...\n                             ...                        \n282    [ঢাকা, গবেষণা, ##পত, ##রে, জালি, ##যা, ##তির, ...\n283    [ঢাকা, অক, ##টে, ##াব, ##রে, ##র, [UNK], তারিখ...\n284    [ঢাকা, ঢাকা, মেডিকেল, কলেজ, ঢামেক, হাসপাতালের,...\n285    [খাগ, ##ডা, ##ছ, ##ডি, পর, ##ধান, ##মন, ##তরী,...\n286    [ঢাকা, আগামী, দ, ##বাদ, ##শ, জাতী, ##য, সংসদ, ...\nName: tokens, Length: 287, dtype: object\n['ফেনী', 'ঢাকা', 'চট', '##র', '##গরা', '##ম', 'মহা', '##সড', '##কে', '##র', 'ফেনীর', 'মহিপাল', 'বাসস', '##ট', '##যান', '##ডে', 'সগ', '##ন', '##ধা', 'পরিবহনের', 'একটি', 'বাসে', 'আগ', '##ন', 'দি', '##যে', '##ছে', 'দর', '##বত', '##তরা', 'রে', '##াব', '##বার', '[UNK]', 'নভ', '##েম', '##বর', 'রাত', '৮', '##টার', 'দিকে', 'এ', 'ঘটনা', 'ঘটে', 'স', '##থান', '##ী', '##যর', '##া', 'জানান', 'মহিপাল', 'মহা', '##সড', '##ক', 'লাগে', '##ায', '##া', 'বাস', 'টার', '##মিনা', '##লের', 'সামনে', 'দা', '##ডান', '##ে', '##া', 'ছিল', '##ে', '##া', 'বাসটি', 'হঠাৎ', 'করে', 'দর', '##বত', '##তরা', 'এসে', 'বাসে', 'আগ', '##ন', 'দি', '##যে', 'চলে', 'যা', '##য', 'খবর', 'পে', '##যে', 'ফায', '##ার', 'সারভিস', 'ঘটনা', '##স', '##থল', '##ে', 'আসার', 'আগেই', 'গা', '##ডির', 'ভেতরে', 'থাকা', 'সিটি', '##সহ', 'অব', '##কাঠ', '##ামে', '##াস', '##হ', 'অধিকাংশ', 'পড', '##ে', 'যা', '##য', 'বাসটির', 'লাইন', '##ম', '##যান', 'আমির', 'হে', '##াসে', '##ন', 'বলেন', 'ঘটনার', 'খবর', 'পে', '##যে', 'সঙ', '##গে', 'সঙ', '##গে', 'আমরা', 'ছট', '##ে', 'আসি', 'কে', 'বা', 'কারা', 'আগ', '##ন', 'দি', '##যে', 'পালি', '##যে', 'যা', '##য', 'মহরত', '##েই', 'সব', 'পড', '##ে', 'যা', '##য', 'বাসটির', 'চালক', 'দাউদ', 'বলেন', 'হরতালের', 'কারণে', 'সারাদিন', 'বাস', 'চালানে', '##া', 'হয', '##নি', 'বাস', 'টার', '##মিনা', '##লেই', 'দা', '##ডি', '##যে', 'ছিল', 'গা', '##ডির', 'বিভিন', '##ন', 'মেরামত', 'কাজ', 'করান', '##ে', '##া', 'হযেছে', 'দা', '##ডান', '##ে', '##া', 'গা', '##ডি', '##তেই', 'অগ', '##নিস', '##ং', '##যে', '##াগ', 'করে', 'পালি', '##যে', '##ছে', 'দর', '##বত', '##তরা', 'ফায', '##ার', 'সারভিস', '##ের', 'সিনি', '##যর', 'স', '##টেশন', 'অফিসার', 'আবদ', '##ল', 'মজিদ', 'বলেন', 'খবর', 'পে', '##যে', '##ই', 'আমরা', 'ছট', '##ে', 'যাই', 'আগ', '##ন', 'নেভাতে', 'চে', '##ষট', '##া', 'করি', 'এ', 'ব', '##যা', '##পারে', 'ফেনী', 'মডেল', 'থানার', 'ভার', '##পরা', '##পত', 'করম', '##কর', '##তা', 'ওসি', 'মে', '##া', 'শহিদ', '##ল', 'ইসলাম', '[UNK]', 'বলেন', 'মহিপাল', 'এলাকা', '##য', 'আমরা', 'নজরদারি', 'বা', '##ডি', '##যে', '##ছি', 'দর', '##বত', '##ত', '##দের', 'চি', '##হন', '##িত', 'করার', 'জন', '##য', 'আশপাশের', 'সিসিটিভি', 'ফ', '##টেজ', 'পাও', '##যা', 'যা', '##যন', '##ি', 'ওই', 'এলাকা', '##য', 'সিসিটিভি', 'ক', '##যাম', '##েরা', 'নেই', 'একটি', 'দে', '##াকা', '##নে', 'ছিল', 'আজ', 'দপ', '##র', 'থেকে', 'সেটিও', 'ন', '##ষট', 'হয', '##ে', 'গেছে', 'বাংলাদেশ', 'সময', '[UNK]', 'ঘণ', '##টা', 'নভ', '##েম', '##বর', '[UNK]', '[UNK]', 'এসএইচ', '##ডি', 'জে', '##এইচ']\nTF-IDF matrix shape: (287, 2702)\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport torch\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split\n\n#max_length=512 \n# Preparing data for model\ndef prepare_data(context, question, answer_text, tokenizer):\n    inputs = tokenizer(question, context, truncation=True, padding='max_length', max_length=100, return_tensors='pt')\n    answer_tokens = tokenizer.encode(answer_text, add_special_tokens=False)\n    try:\n        start_position = inputs.input_ids.squeeze().tolist().index(answer_tokens[0])\n    except ValueError:\n        return None\n    end_position = start_position + len(answer_tokens) - 1\n    return {\n        'input_ids': inputs['input_ids'].squeeze().tolist(),\n        'attention_mask': inputs['attention_mask'].squeeze().tolist(),\n        'start_positions': start_position,\n        'end_positions': end_position\n    }\n\n# Example questions and answers (replace these with actual questions and answers)\nquestions = [\"Example question for the context\"] * len(subset_data)  # Replace with actual questions\nanswers = [\"Example answer text\"] * len(subset_data)  # Replace with actual answers\n\n# Prepare dataset\ndataset = []\nfor i in range(len(subset_data)):\n    context = subset_data['cleaned'].iloc[i]\n    question = questions[i]\n    answer_text = answers[i]\n    prepared_data = prepare_data(context, question, answer_text, bnbert_tokenizer)\n    if prepared_data:\n        dataset.append(prepared_data)\n\n ","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:02:24.232762Z","iopub.execute_input":"2024-10-01T16:02:24.233632Z","iopub.status.idle":"2024-10-01T16:02:25.084681Z","shell.execute_reply.started":"2024-10-01T16:02:24.233595Z","shell.execute_reply":"2024-10-01T16:02:25.083727Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nclass QADataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    \n# Split the dataset\ntrain_size = int(0.8 * len(dataset))\ntrain_dataset = dataset[:train_size]\neval_dataset = dataset[train_size:]\n\ntrain_encodings = {\n    'input_ids': [d['input_ids'] for d in train_dataset],\n    'attention_mask': [d['attention_mask'] for d in train_dataset],\n    'start_positions': [d['start_positions'] for d in train_dataset],\n    'end_positions': [d['end_positions'] for d in train_dataset],\n}\n\neval_encodings = {\n    'input_ids': [d['input_ids'] for d in eval_dataset],\n    'attention_mask': [d['attention_mask'] for d in eval_dataset],\n    'start_positions': [d['start_positions'] for d in eval_dataset],\n    'end_positions': [d['end_positions'] for d in eval_dataset],\n}\n\ntrain_dataset = QADataset(train_encodings)\neval_dataset = QADataset(eval_encodings)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:02:29.429493Z","iopub.execute_input":"2024-10-01T16:02:29.429876Z","iopub.status.idle":"2024-10-01T16:02:29.440245Z","shell.execute_reply.started":"2024-10-01T16:02:29.429845Z","shell.execute_reply":"2024-10-01T16:02:29.438986Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Load the pre-trained model\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"sagorsarker/bangla-bert-base\")\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',           \n    num_train_epochs=8,               \n    per_device_train_batch_size=4,    \n    per_device_eval_batch_size=4,     \n    warmup_steps=500,                 \n    weight_decay=0.01,                \n    logging_dir='./logs',            \n    logging_steps=10,\n)\n\n# Create the Trainer\ntrainer = Trainer(\n    model=model,                          \n    args=training_args,                   \n    train_dataset=train_dataset,          \n    eval_dataset=eval_dataset             \n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T16:02:32.097783Z","iopub.execute_input":"2024-10-01T16:02:32.098913Z","iopub.status.idle":"2024-10-01T16:05:01.341338Z","shell.execute_reply.started":"2024-10-01T16:02:32.098871Z","shell.execute_reply":"2024-10-01T16:05:01.339415Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/660M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b6ffef349a143c8a9e4b49f7b9a530c"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241001_160340-u7stqg3u</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/chatbotfydp/huggingface/runs/u7stqg3u' target=\"_blank\">twilight-sun-24</a></strong> to <a href='https://wandb.ai/chatbotfydp/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/chatbotfydp/huggingface' target=\"_blank\">https://wandb.ai/chatbotfydp/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/chatbotfydp/huggingface/runs/u7stqg3u' target=\"_blank\">https://wandb.ai/chatbotfydp/huggingface/runs/u7stqg3u</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 15/464 00:55 < 31:58, 0.23 it/s, Epoch 0.24/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>4.507400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                          \n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                   \n\u001b[1;32m     20\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,          \n\u001b[1;32m     21\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset             \n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model.save_pretrained('./fine-tuned-bangla-bert')\nbnbert_tokenizer.save_pretrained('./fine-tuned-bangla-bert')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:42:23.354850Z","iopub.execute_input":"2024-06-03T05:42:23.355294Z","iopub.status.idle":"2024-06-03T05:42:24.541890Z","shell.execute_reply.started":"2024-06-03T05:42:23.355263Z","shell.execute_reply":"2024-06-03T05:42:24.540419Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('./fine-tuned-bangla-bert/tokenizer_config.json',\n './fine-tuned-bangla-bert/special_tokens_map.json',\n './fine-tuned-bangla-bert/vocab.txt',\n './fine-tuned-bangla-bert/added_tokens.json',\n './fine-tuned-bangla-bert/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"!pip install rouge-score\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:43:15.445923Z","iopub.execute_input":"2024-06-03T05:43:15.447185Z","iopub.status.idle":"2024-06-03T05:43:31.901064Z","shell.execute_reply.started":"2024-06-03T05:43:15.447135Z","shell.execute_reply":"2024-06-03T05:43:31.899539Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=d1f35e6659bf1977076113a4a054a14d1b28efadbbdb08f515ce9a0c421b1d82\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:43:41.985032Z","iopub.execute_input":"2024-06-03T05:43:41.985449Z","iopub.status.idle":"2024-06-03T05:43:41.993579Z","shell.execute_reply.started":"2024-06-03T05:43:41.985420Z","shell.execute_reply":"2024-06-03T05:43:41.992188Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained('./fine-tuned-bangla-bert')\ntokenizer = AutoTokenizer.from_pretrained('./fine-tuned-bangla-bert')\n\ndef process_text(Text):\n    Text = re.sub('[^\\u0980-\\u09FF]', ' ', str(Text))\n    return Text\n\n \ndef find_relevant_context(question, tfidf_matrix, vectorizer, data):\n    question_tfidf = vectorizer.transform([question])\n    cosine_similarities = (tfidf_matrix * question_tfidf.T).toarray()\n    most_relevant_index = cosine_similarities.argmax()\n    return data.iloc[most_relevant_index]['cleaned']\n\ndef get_answer_with_beam_search(question, context, tokenizer, model, beam_size=5, max_length=100):\n    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        start_logits = outputs.start_logits.squeeze()\n        end_logits = outputs.end_logits.squeeze()\n        start_index = start_logits.argmax().item()\n        end_index = end_logits.argmax().item()\n        relevant_context = context[start_index:end_index]\n        inputs = tokenizer(question, relevant_context, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length)\n        outputs = model(**inputs)\n        start_logits = outputs.start_logits.squeeze()\n        end_logits = outputs.end_logits.squeeze()\n        topk_start_indices = torch.topk(start_logits, beam_size).indices\n        topk_end_indices = torch.topk(end_logits, beam_size).indices\n        current_sequences = [(start_index.item(), start_logits[start_index].item(), None) for start_index in topk_start_indices]\n        best_score = float('-inf')\n        while True:\n            all_candidates = []\n            for current_index, current_score, prev_index in current_sequences:\n                for end_index in topk_end_indices:\n                    if end_index.item() < current_index:\n                        continue\n                    new_score = current_score + end_logits[end_index].item()\n                    all_candidates.append((end_index.item(), new_score, current_index))\n            if not all_candidates:\n                break\n            sorted_candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n            max_score = sorted_candidates[0][1] if sorted_candidates else float('-inf')\n            if max_score <= best_score:\n                break\n            best_score = max_score\n            current_sequences = sorted_candidates\n            best_sequence = max(current_sequences, key=lambda x: x[1])\n            best_end_index = best_sequence[0]\n            if best_end_index is not None:\n                break\n        answer_indices = [best_end_index]\n        prev_index = best_sequence[2]\n        while prev_index is not None:\n            answer_indices.append(prev_index)\n            prev_index = next((seq[2] for seq in current_sequences if seq[0] == prev_index), None)\n        answer_indices.reverse()\n        answer_tokens = inputs.input_ids[0][answer_indices[0]:answer_indices[-1] + 1]\n        answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n    return answer.strip()\n\n\n# def get_answer_with_beam_search(question, context, beam_size=5, max_length=512):\n#     inputs = tokenizer(question, context, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n#     with torch.no_grad():\n#         outputs = model(**inputs)\n\n#     start_logits = outputs.start_logits.squeeze()\n#     end_logits = outputs.end_logits.squeeze()\n\n#     start_indices = torch.topk(start_logits, beam_size).indices\n#     end_indices = torch.topk(end_logits, beam_size).indices\n\n#     best_start = None\n#     best_end = None\n#     best_score = -float('inf')\n\n#     for start_index in start_indices:\n#         for end_index in end_indices:\n#             if end_index >= start_index:\n#                 score = start_logits[start_index] + end_logits[end_index]\n#                 if score > best_score:\n#                     best_start = start_index\n#                     best_end = end_index\n#                     best_score = score\n\n#     answer_tokens = inputs.input_ids[0][best_start:best_end + 1]\n#     answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n\n#     return answer.strip()\n\n\n \n\n \n \n# vectorizer = TfidfVectorizer()\n# tfidf_matrix = vectorizer.fit_transform(subset_data['cleaned'])\n\n# Load evaluation data\n# evaluation_data = pd.read_excel('/kaggle/input/eval-dataset/Eval_Dataset.xlsx')\n\n# evaluation_data['Context'] = evaluation_data['Context'].apply(process_text)\n# evaluation_data['Question'] = evaluation_data[' Question '].apply(process_text)\n# evaluation_data['Answer'] = evaluation_data['Answer'].apply(process_text)\n\n\n# # Evaluate the model\n# em_scores = []\n# rouge_scores = []\n# bleu_scores = []\n\n# # Initialize ROUGE scorer\n# rouge_scorer_inst = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n# smoothie = SmoothingFunction().method4\n\n# for index, row in evaluation_data.iterrows():\n#     context = row['Context']\n#     question = row['Question']\n#     true_answer = row['Answer']\n\n#     # Find relevant context and get the predicted answer\n#     relevant_context = find_relevant_context(question, tfidf_matrix, vectorizer, subset_data)\n#     predicted_answer = get_answer_with_beam_search(question, relevant_context)\n\n#     # Calculate Exact Match (EM)\n#     em_score = 1 if predicted_answer.strip() == true_answer.strip() else 0\n#     em_scores.append(em_score)\n\n#     # Calculate ROUGE Score\n#     rouge_score = rouge_scorer_inst.score(true_answer, predicted_answer)['rougeL'].fmeasure\n#     rouge_scores.append(rouge_score)\n\n#     # Calculate BLEU Score\n#     bleu_score = sentence_bleu([true_answer.split()], predicted_answer.split(), smoothing_function=smoothie)\n#     bleu_scores.append(bleu_score)\n\n# # Calculate and print the average scores\n# if em_scores:\n#     avg_em = sum(em_scores) / len(em_scores)\n# else:\n#     avg_em = 0\n    \n# if rouge_scores:\n#     avg_rouge = sum(rouge_scores) / len(rouge_scores)\n# else:\n#     avg_rouge = 0\n    \n# if bleu_scores:\n#     avg_bleu = sum(bleu_scores) / len(bleu_scores)\n# else:\n#     avg_bleu = 0\n\n# print(f\"Average EM Score: {avg_em}\")\n# print(f\"Average ROUGE Score: {avg_rouge}\")\n# print(f\"Average BLEU Score: {avg_bleu}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:43:43.938166Z","iopub.execute_input":"2024-06-03T05:43:43.938577Z","iopub.status.idle":"2024-06-03T05:43:44.660035Z","shell.execute_reply.started":"2024-06-03T05:43:43.938543Z","shell.execute_reply":"2024-06-03T05:43:44.658641Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"eval_data = pd.read_excel('/kaggle/input/eval-dataset/Eval_Dataset.xlsx')\n# print(eval_data.head())\ndef process_text(Text):\n    Text = re.sub('[^\\u0980-\\u09FF]', ' ', str(Text))\n    return Text\n\n# Apply the cleaning function to the Context column\neval_data['cleaned_context'] = eval_data['Context'].apply(process_text)\neval_data['Answer'].fillna('', inplace=True)\n\neval_data[' Question '] = eval_data[' Question '].astype(str)\neval_data['cleaned_context'] = eval_data['cleaned_context'].astype(str)\n\n# # tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n\n# # # Tokenize the cleaned context\n# # eval_data['tokens'] = eval_data['cleaned_context'].apply(lambda x: tokenizer.tokenize(x))\n\n# print(eval_data[['cleaned_context', 'tokens']].head())","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:43:49.198672Z","iopub.execute_input":"2024-06-03T05:43:49.199229Z","iopub.status.idle":"2024-06-03T05:43:49.749928Z","shell.execute_reply.started":"2024-06-03T05:43:49.199187Z","shell.execute_reply":"2024-06-03T05:43:49.748504Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/4207749966.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  eval_data['Answer'].fillna('', inplace=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge-score\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:43:55.245764Z","iopub.execute_input":"2024-06-03T05:43:55.246628Z","iopub.status.idle":"2024-06-03T05:44:08.920557Z","shell.execute_reply.started":"2024-06-03T05:43:55.246587Z","shell.execute_reply":"2024-06-03T05:44:08.918980Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: rouge-score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install pandas transformers torch rouge-score nltk\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:44:13.765199Z","iopub.execute_input":"2024-06-03T05:44:13.765638Z","iopub.status.idle":"2024-06-03T05:44:27.086374Z","shell.execute_reply.started":"2024-06-03T05:44:13.765601Z","shell.execute_reply":"2024-06-03T05:44:27.084931Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nRequirement already satisfied: rouge-score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install rouge\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:44:36.312349Z","iopub.execute_input":"2024-06-03T05:44:36.312793Z","iopub.status.idle":"2024-06-03T05:44:49.882087Z","shell.execute_reply.started":"2024-06-03T05:44:36.312739Z","shell.execute_reply":"2024-06-03T05:44:49.880545Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom rouge import Rouge\nfrom nltk.translate.bleu_score import sentence_bleu\nmodel = AutoModelForQuestionAnswering.from_pretrained('./fine-tuned-bangla-bert')\n\n# Define a function to get answers from the model\ndef get_answer(question, context):\n    if not isinstance(question, str) or not isinstance(context, str):\n        raise ValueError(\"Both question and context must be strings.\")\n    # Tokenize the inputs with truncation to the maximum length of 512\n    inputs = tokenizer(question, context, max_length=512, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n    answer_start = torch.argmax(outputs.start_logits)\n    answer_end = torch.argmax(outputs.end_logits) + 1\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][answer_start:answer_end]))\n    return answer\n# Generate predictions\neval_data['predicted_answer'] = eval_data.apply(lambda row: get_answer(row[' Question '], row['cleaned_context']), axis=1)\n\n# Filter out rows where the predicted answer or the actual answer is empty\neval_data_non_empty = eval_data[(eval_data['predicted_answer'].str.strip() != '') & (eval_data['Answer'].str.strip() != '')]\n\n# Display the first few rows of the result\nprint(eval_data_non_empty[[' Question ', 'cleaned_context', 'Answer', 'predicted_answer']].head())\n\n# Calculate Exact Match (EM) score\ndef exact_match(pred, true):\n    return int(pred == true)\n\neval_data_non_empty['em'] = eval_data_non_empty.apply(lambda row: exact_match(row['predicted_answer'], row['Answer']), axis=1)\nem_score = eval_data_non_empty['em'].mean()\nprint(f'Exact Match (EM) score: {em_score}')\n\n# Calculate ROUGE score\nfrom rouge import Rouge\nrouge = Rouge()\nscores = rouge.get_scores(eval_data_non_empty['predicted_answer'].tolist(), eval_data_non_empty['Answer'].tolist(), avg=True)\nprint(f'ROUGE score: {scores}')\n\n# Calculate BLEU score\nfrom nltk.translate.bleu_score import sentence_bleu\n\ndef bleu_score(reference, candidate):\n    reference = [reference.split()]\n    candidate = candidate.split()\n    return sentence_bleu(reference, candidate)\n\neval_data_non_empty['bleu'] = eval_data_non_empty.apply(lambda row: bleu_score(row['Answer'], row['predicted_answer']), axis=1)\nbleu_score_avg = eval_data_non_empty['bleu'].mean()\nprint(f'BLEU score: {bleu_score_avg}')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:44:52.278835Z","iopub.execute_input":"2024-06-03T05:44:52.279414Z","iopub.status.idle":"2024-06-03T05:46:09.655069Z","shell.execute_reply.started":"2024-06-03T05:44:52.279374Z","shell.execute_reply":"2024-06-03T05:46:09.653858Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"                                           Question   \\\n1          কোন ধরনের প্রতিষ্ঠানে আগুন লাগানো হয়েছে?   \n2                                 কখন এই ঘটনা ঘটেছে?   \n3                      কোন বিদ্যালয়ে এই ঘটনা ঘটেছে?   \n5                 কত স্থানে অগ্নিসংযোগের ঘটনা ঘটেছে?   \n6  কোন সময়কালের মধ্যে এই অগ্নিসংযোগের ঘটনাগুলি ঘ...   \n\n                                     cleaned_context  \\\n1  মানিকগঞ্জ মানিকগঞ্জের ঘিওর উপজেলার বালিয়াখোড়া ...   \n2  মানিকগঞ্জ  মানিকগঞ্জের ঘিওর উপজেলার বালিয়াখোড়া...   \n3  মানিকগঞ্জ মানিকগঞ্জের ঘিওর উপজেলার বালিয়াখোড়া ...   \n5  ঢাকা  দ্বাদশ জাতীয় সংসদ নির্বাচনের তফসিল ঘোষণা...   \n6  ঢাকা  দ্বাদশ জাতীয় সংসদ নির্বাচনের তফসিল ঘোষণা...   \n\n                                              Answer  \\\n1  একটি প্রাথমিক বিদ্যালয়ের অফিস কক্ষের পাশের বা...   \n2        বৃহস্পতিবার, ১৬ নভেম্বর ভোরে এই ঘটনা ঘটেছে।   \n3  ১৫ নম্বর সরকারি প্রাথমিক বিদ্যালয়ে এই ঘটনা ঘট...   \n5      দেশজুড়ে ১১টি স্থানে অগ্নিসংযোগের ঘটনা ঘটেছে।   \n6  এই অগ্নিসংযোগের ঘটনাগুলি বুধবার, ১৫ নভেম্বর সন...   \n\n                         predicted_answer  \n1  কোন ধরনের পরতিষঠানে আগন লাগানো হযেছে  \n2                কখন এই ঘটনা ঘটেছে? [SEP]  \n3                                   [SEP]  \n5                                   [SEP]  \n6                                   ##গলি  \nExact Match (EM) score: 0.0\nROUGE score: {'rouge-1': {'r': 0.14290950804619826, 'p': 0.07599561067495109, 'f': 0.07240299591013694}, 'rouge-2': {'r': 0.036626710484704526, 'p': 0.012027363167160552, 'f': 0.014817328312689951}, 'rouge-l': {'r': 0.13396069623141876, 'p': 0.07228967053260733, 'f': 0.06771191076740103}}\nBLEU score: 0.17906784829694727\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/1781009302.py:31: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  eval_data_non_empty['em'] = eval_data_non_empty.apply(lambda row: exact_match(row['predicted_answer'], row['Answer']), axis=1)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/tmp/ipykernel_33/1781009302.py:49: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  eval_data_non_empty['bleu'] = eval_data_non_empty.apply(lambda row: bleu_score(row['Answer'], row['predicted_answer']), axis=1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(data.loc[220,'Text'])","metadata":{"execution":{"iopub.status.busy":"2024-06-01T17:32:28.610784Z","iopub.execute_input":"2024-06-01T17:32:28.611831Z","iopub.status.idle":"2024-06-01T17:32:28.618451Z","shell.execute_reply.started":"2024-06-01T17:32:28.611792Z","shell.execute_reply":"2024-06-01T17:32:28.616908Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#Ask a question and get the answer\n\n#question = \"চাঁদপুর কত কোটি টাকা ব্যয়ে নির্মাণ করা হয়েছে ৫ কিলোমিটার ড্রেন ?\"\n#question = \"বিএনপি?\"\n#question = \" নারায়ণগঞ্জের রূপগঞ্জে\"\n#question = \"ঢাকা মেট্রোপলিটন পুলিশের  ডিএমপি\"\n#question = \"ঘিওর উপজেলার?\"\n#question = \"মিরিকপুর গ্রাম কোথায়?\"\n#question = \"শাহমাহমুদপুর ইউনিয়ন কোথায়\"\n#question = \"পররাষ্ট্রমন্ত্রী এ কে আব্দুল মোমেন\" \n# question = \"নীলফামারীর জলঢাকায়\" loc[80\n#question = \"প্রধানমন্ত্রী শেখ হাসিনার\"\n# question = \"জয়পুরহাটের কালাই উপজেলা\"  #52\n# question = \"ফরিদপুরের কানাইপুর?\"\n#question = \"ফিচার লেখক সমুদ্র হক এ এস এম খবিরুল হক?\"\n#question = \"রাজশাহী সিটি মেয়র ?\"\n#question = \"পররাষ্ট্রমন্ত্রী কে?\"\nquestion = \"রাজধানীতে মাদকবিরোধী অভিযান চালিয়ে কতজনকে  জনকে গ্রেপ্তার করেছে পুলিশ?\" \n#question = \"রাঙামাটি কোতোয়ালি থানার ভারপ্রাপ্ত কর্মকর্তা\" \n\n#question = \"বাংলাদেশ বিশ্ববিদ্যালয় মঞ্জুরি কমিশন (ইউজিসি) এর সদস্য ও ইআরডিএফবির সভাপতি কে?\"\n#question = \"ঢাকা জেলার দোহার বাজারে কোথায় আগুন লেগেছিলো ?\"\n\ncontext = find_relevant_context(question, tfidf_matrix, vectorizer, subset_data)\nprint(\"Relevant Answer:\", context)\n\n# answer = get_answer_with_beam_search(question, context, tokenizer, model)\n# print(\"Answer:\", answer)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T06:38:27.238167Z","iopub.execute_input":"2024-06-03T06:38:27.238633Z","iopub.status.idle":"2024-06-03T06:38:27.257258Z","shell.execute_reply.started":"2024-06-03T06:38:27.238596Z","shell.execute_reply":"2024-06-03T06:38:27.255524Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Relevant Answer: ঢাকা  রাজধানীতে মাদকবিরোধী অভিযানে বিক্রি ও সেবনের অভিযোগে ২২ জনকে গ্রেপ্তার করেছে ঢাকা মেট্রোপলিটন পুলিশ  ডিএমপি    বুধবার  ১৫ নভেম্বর  সকাল ৬টা থেকে বৃহস্পতিবার  ১৬ নভেম্বর  সকাল ৬টা পর্যন্ত রাজধানীর বিভিন্ন থানা এলাকায় অভিযান চালিয়ে তাদের গ্রেপ্তার করা হয়          ডিএমপির মিডিয়া অ্যান্ড পাবলিক রিলেশনস বিভাগের অতিরিক্ত উপ কমিশনার  এডিসি  কে এন রায় নিয়তি জানান  আসামিদের কাছ থেকে ৮৬৩ পিস ইয়াবা  ১০ কেজি ৫০০ গ্রাম গাঁজা  ২০ গ্রাম হেরোইন ও ১৫টি ট্যাপেন্টাডল ট্যাবলেট জব্দ করা হয়   আসামিদের বিরুদ্ধে ডিএমপির থানায় মাদকদ্রব্য নিয়ন্ত্রণ আইনে ১৯টি মামলা দায়ের করা হয়েছে বলে জানান তিনি   বাংলাদেশ সময়  ১০৪১ ঘণ্টা  নভেম্বর ১৬  ২০২৩ এমএমআই আরআইএস\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(data.loc[320,'Text'])","metadata":{"execution":{"iopub.status.busy":"2024-06-01T19:57:53.366761Z","iopub.execute_input":"2024-06-01T19:57:53.368388Z","iopub.status.idle":"2024-06-01T19:57:53.376171Z","shell.execute_reply.started":"2024-06-01T19:57:53.368330Z","shell.execute_reply":"2024-06-01T19:57:53.374656Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# eval_results = trainer.evaluate()\n# print(\"Evaluation results:\", eval_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# eval_results = trainer.evaluate()\n\n# # Print evaluation results\n# print(\"Evaluation results:\", eval_results)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T16:01:34.640715Z","iopub.status.idle":"2024-05-27T16:01:34.641108Z","shell.execute_reply.started":"2024-05-27T16:01:34.640916Z","shell.execute_reply":"2024-05-27T16:01:34.640933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}